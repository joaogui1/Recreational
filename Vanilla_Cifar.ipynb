{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla_Cifar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8vxsLemzZAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MJwAHpE9Ews",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd9b4de4-68af-4439-f510-4528bbbbf883"
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_pUCikx068y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivHcbORv1TeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subtract_pixel_mean = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbtLpfuR1WLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 2\n",
        "depth = n * 9 + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_hEH6f41lrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_type = f'ResNet{depth}v2'\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POv0to3513CA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "15ab5bbb-e03e-4f19-a9dc-46d6e88a9c8d"
      },
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpYbBHhp18cU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVfTDdti2All",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKn7DAze2MXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVe_k_Ax2RcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_no_BN(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     batch_normalization=False,\n",
        "                     conv_first=True)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=False,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             batch_normalization=False,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             batch_normalization=False,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # # Add classifier on top.\n",
        "\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a15TdNQC2ikk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2727c941-6028-4d69-c83d-1e75411a041d"
      },
      "source": [
        "model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqwuc9o64Cbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33acb664-d4e6-4e88-cccf-0d05021a0075"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   272         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 64)   1088        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 64)   0           conv2d_4[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   1040        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   1088        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           add[0][0]                        \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 64)   4160        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36928       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  8320        add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  8320        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 128)  0           conv2d_11[0][0]                  \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 128)  512         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 128)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 64)   8256        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 128)  0           add_2[0][0]                      \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 128)    16512       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 128)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 128)    147584      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 128)    512         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 128)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 256)    33024       add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 256)    33024       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 256)    0           conv2d_18[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 256)    1024        add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 256)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 128)    32896       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 128)    512         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 128)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 128)    147584      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 128)    512         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 128)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 256)    33024       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 256)    0           add_4[0][0]                      \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 256)    1024        add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 256)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 256)    0           activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 256)          0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           2570        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 574,090\n",
            "Trainable params: 570,602\n",
            "Non-trainable params: 3,488\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irMfjPNA5g4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [lr_reducer, lr_scheduler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpilH5u95s51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25dde7df-adda-4b8e-fd4a-947c7ab1e2e9"
      },
      "source": [
        "print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74ZUcAVA57cp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f473b106-565e-4031-f95a-e63b54c91ea3"
      },
      "source": [
        "datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=1,\n",
        "                        callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n",
            "Epoch 1/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.7672 - acc: 0.4842Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 527us/sample - loss: 1.7460 - acc: 0.4601\n",
            "1563/1563 [==============================] - 97s 62ms/step - loss: 1.7671 - acc: 0.4843 - val_loss: 1.8517 - val_acc: 0.4601\n",
            "Learning rate:  0.001\n",
            "Epoch 2/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.3402 - acc: 0.6238Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 472us/sample - loss: 1.8874 - acc: 0.5016\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 1.3402 - acc: 0.6238 - val_loss: 1.9035 - val_acc: 0.5016\n",
            "Learning rate:  0.001\n",
            "Epoch 3/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.1648 - acc: 0.6787Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 471us/sample - loss: 1.0773 - acc: 0.6901\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 1.1646 - acc: 0.6787 - val_loss: 1.1404 - val_acc: 0.6901\n",
            "Learning rate:  0.001\n",
            "Epoch 4/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0586 - acc: 0.7177Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 472us/sample - loss: 0.9738 - acc: 0.7052\n",
            "1563/1563 [==============================] - 84s 53ms/step - loss: 1.0585 - acc: 0.7177 - val_loss: 1.0811 - val_acc: 0.7052\n",
            "Learning rate:  0.001\n",
            "Epoch 5/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.9880 - acc: 0.7369Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 471us/sample - loss: 1.0410 - acc: 0.6650\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.9879 - acc: 0.7369 - val_loss: 1.2568 - val_acc: 0.6650\n",
            "Learning rate:  0.001\n",
            "Epoch 6/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.9370 - acc: 0.7550Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 470us/sample - loss: 0.9126 - acc: 0.6950\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.9368 - acc: 0.7550 - val_loss: 1.1297 - val_acc: 0.6950\n",
            "Learning rate:  0.001\n",
            "Epoch 7/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.8909 - acc: 0.7708Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 470us/sample - loss: 0.9506 - acc: 0.7332\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.8910 - acc: 0.7708 - val_loss: 1.0152 - val_acc: 0.7332\n",
            "Learning rate:  0.001\n",
            "Epoch 8/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.8604 - acc: 0.7821Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 472us/sample - loss: 0.9997 - acc: 0.7402\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.8604 - acc: 0.7821 - val_loss: 1.0129 - val_acc: 0.7402\n",
            "Learning rate:  0.001\n",
            "Epoch 9/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.8335 - acc: 0.7926Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 466us/sample - loss: 0.8372 - acc: 0.7770\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.8335 - acc: 0.7926 - val_loss: 0.8979 - val_acc: 0.7770\n",
            "Learning rate:  0.001\n",
            "Epoch 10/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.8120 - acc: 0.8000Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 466us/sample - loss: 0.8096 - acc: 0.7707\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.8119 - acc: 0.7999 - val_loss: 0.8959 - val_acc: 0.7707\n",
            "Learning rate:  0.001\n",
            "Epoch 11/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7917 - acc: 0.8060Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 467us/sample - loss: 0.9658 - acc: 0.7546\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.7917 - acc: 0.8060 - val_loss: 0.9529 - val_acc: 0.7546\n",
            "Learning rate:  0.001\n",
            "Epoch 12/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7759 - acc: 0.8105Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 466us/sample - loss: 0.6688 - acc: 0.7939\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.7758 - acc: 0.8105 - val_loss: 0.8145 - val_acc: 0.7939\n",
            "Learning rate:  0.001\n",
            "Epoch 13/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7614 - acc: 0.8150Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 487us/sample - loss: 0.9467 - acc: 0.7521\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.7616 - acc: 0.8150 - val_loss: 0.9968 - val_acc: 0.7521\n",
            "Learning rate:  0.001\n",
            "Epoch 14/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7431 - acc: 0.8218Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 466us/sample - loss: 0.8853 - acc: 0.7373\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.7431 - acc: 0.8218 - val_loss: 1.1447 - val_acc: 0.7373\n",
            "Learning rate:  0.001\n",
            "Epoch 15/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7346 - acc: 0.8224Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 466us/sample - loss: 1.0757 - acc: 0.7057\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.7348 - acc: 0.8223 - val_loss: 1.2297 - val_acc: 0.7057\n",
            "Learning rate:  0.001\n",
            "Epoch 16/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7238 - acc: 0.8271Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 461us/sample - loss: 0.7871 - acc: 0.7675\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.7236 - acc: 0.8271 - val_loss: 0.9277 - val_acc: 0.7675\n",
            "Learning rate:  0.001\n",
            "Epoch 17/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7133 - acc: 0.8302Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 463us/sample - loss: 0.8914 - acc: 0.7365\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.7133 - acc: 0.8302 - val_loss: 1.0676 - val_acc: 0.7365\n",
            "Learning rate:  0.001\n",
            "Epoch 18/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.7050 - acc: 0.8324Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 466us/sample - loss: 0.7925 - acc: 0.7889\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 0.7049 - acc: 0.8324 - val_loss: 0.8461 - val_acc: 0.7889\n",
            "Learning rate:  0.001\n",
            "Epoch 19/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6964 - acc: 0.8366Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 464us/sample - loss: 0.7515 - acc: 0.7734\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 0.6964 - acc: 0.8366 - val_loss: 0.8904 - val_acc: 0.7734\n",
            "Learning rate:  0.001\n",
            "Epoch 20/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6834 - acc: 0.8409Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 464us/sample - loss: 0.9917 - acc: 0.7820\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 0.6834 - acc: 0.8409 - val_loss: 0.9173 - val_acc: 0.7820\n",
            "Learning rate:  0.001\n",
            "Epoch 21/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6761 - acc: 0.8432Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 513us/sample - loss: 0.8219 - acc: 0.8057\n",
            "1563/1563 [==============================] - 89s 57ms/step - loss: 0.6760 - acc: 0.8432 - val_loss: 0.8153 - val_acc: 0.8057\n",
            "Learning rate:  0.001\n",
            "Epoch 22/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6712 - acc: 0.8442Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 512us/sample - loss: 0.7741 - acc: 0.7783\n",
            "1563/1563 [==============================] - 96s 62ms/step - loss: 0.6713 - acc: 0.8442 - val_loss: 0.9540 - val_acc: 0.7783\n",
            "Learning rate:  0.001\n",
            "Epoch 23/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6609 - acc: 0.8474Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 523us/sample - loss: 0.7163 - acc: 0.8028\n",
            "1563/1563 [==============================] - 98s 63ms/step - loss: 0.6608 - acc: 0.8474 - val_loss: 0.8329 - val_acc: 0.8028\n",
            "Learning rate:  0.001\n",
            "Epoch 24/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6571 - acc: 0.8486Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 521us/sample - loss: 0.8513 - acc: 0.7976\n",
            "1563/1563 [==============================] - 101s 65ms/step - loss: 0.6571 - acc: 0.8486 - val_loss: 0.8508 - val_acc: 0.7976\n",
            "Learning rate:  0.001\n",
            "Epoch 25/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6490 - acc: 0.8525Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 546us/sample - loss: 0.8382 - acc: 0.7935\n",
            "1563/1563 [==============================] - 101s 65ms/step - loss: 0.6490 - acc: 0.8525 - val_loss: 0.8526 - val_acc: 0.7935\n",
            "Learning rate:  0.001\n",
            "Epoch 26/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6453 - acc: 0.8524Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 529us/sample - loss: 0.6564 - acc: 0.7963\n",
            "1563/1563 [==============================] - 100s 64ms/step - loss: 0.6453 - acc: 0.8524 - val_loss: 0.8426 - val_acc: 0.7963\n",
            "Learning rate:  0.001\n",
            "Epoch 27/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6363 - acc: 0.8553Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 530us/sample - loss: 0.6605 - acc: 0.8135\n",
            "1563/1563 [==============================] - 100s 64ms/step - loss: 0.6362 - acc: 0.8553 - val_loss: 0.7771 - val_acc: 0.8135\n",
            "Learning rate:  0.001\n",
            "Epoch 28/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6404 - acc: 0.8537Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 530us/sample - loss: 0.8718 - acc: 0.7844\n",
            "1563/1563 [==============================] - 102s 65ms/step - loss: 0.6403 - acc: 0.8537 - val_loss: 0.9275 - val_acc: 0.7844\n",
            "Learning rate:  0.001\n",
            "Epoch 29/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6313 - acc: 0.8585Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 504us/sample - loss: 0.7966 - acc: 0.8059\n",
            "1563/1563 [==============================] - 102s 65ms/step - loss: 0.6316 - acc: 0.8584 - val_loss: 0.8435 - val_acc: 0.8059\n",
            "Learning rate:  0.001\n",
            "Epoch 30/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6282 - acc: 0.8607Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 474us/sample - loss: 0.9692 - acc: 0.7620\n",
            "1563/1563 [==============================] - 90s 58ms/step - loss: 0.6282 - acc: 0.8607 - val_loss: 0.9899 - val_acc: 0.7620\n",
            "Learning rate:  0.001\n",
            "Epoch 31/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6243 - acc: 0.8603Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 464us/sample - loss: 0.7296 - acc: 0.7998\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.6243 - acc: 0.8603 - val_loss: 0.8224 - val_acc: 0.7998\n",
            "Learning rate:  0.001\n",
            "Epoch 32/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6209 - acc: 0.8622Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 464us/sample - loss: 0.7116 - acc: 0.8278\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.6210 - acc: 0.8621 - val_loss: 0.7484 - val_acc: 0.8278\n",
            "Learning rate:  0.001\n",
            "Epoch 33/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6178 - acc: 0.8617Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 461us/sample - loss: 0.6879 - acc: 0.8262\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 0.6178 - acc: 0.8617 - val_loss: 0.7317 - val_acc: 0.8262\n",
            "Learning rate:  0.001\n",
            "Epoch 34/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6175 - acc: 0.8622Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 463us/sample - loss: 0.8460 - acc: 0.8164\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 0.6175 - acc: 0.8622 - val_loss: 0.7601 - val_acc: 0.8164\n",
            "Learning rate:  0.001\n",
            "Epoch 35/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6075 - acc: 0.8659Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 463us/sample - loss: 1.2072 - acc: 0.7360\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 0.6074 - acc: 0.8660 - val_loss: 1.1434 - val_acc: 0.7360\n",
            "Learning rate:  0.001\n",
            "Epoch 36/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6068 - acc: 0.8653Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 465us/sample - loss: 0.6485 - acc: 0.8298\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 0.6068 - acc: 0.8653 - val_loss: 0.7519 - val_acc: 0.8298\n",
            "Learning rate:  0.001\n",
            "Epoch 37/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.6015 - acc: 0.8677Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 469us/sample - loss: 0.8728 - acc: 0.8131\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.6015 - acc: 0.8677 - val_loss: 0.8182 - val_acc: 0.8131\n",
            "Learning rate:  0.001\n",
            "Epoch 38/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5947 - acc: 0.8698Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 467us/sample - loss: 0.7988 - acc: 0.8316\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.5946 - acc: 0.8698 - val_loss: 0.7321 - val_acc: 0.8316\n",
            "Learning rate:  0.001\n",
            "Epoch 39/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5969 - acc: 0.8693Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 457us/sample - loss: 0.7344 - acc: 0.8009\n",
            "1563/1563 [==============================] - 85s 55ms/step - loss: 0.5970 - acc: 0.8693 - val_loss: 0.8440 - val_acc: 0.8009\n",
            "Learning rate:  0.001\n",
            "Epoch 40/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5972 - acc: 0.8701Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 459us/sample - loss: 0.8818 - acc: 0.7905\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.5973 - acc: 0.8701 - val_loss: 0.8943 - val_acc: 0.7905\n",
            "Learning rate:  0.001\n",
            "Epoch 41/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5883 - acc: 0.8714Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 467us/sample - loss: 0.8982 - acc: 0.8213\n",
            "1563/1563 [==============================] - 85s 55ms/step - loss: 0.5881 - acc: 0.8714 - val_loss: 0.7644 - val_acc: 0.8213\n",
            "Learning rate:  0.001\n",
            "Epoch 42/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5879 - acc: 0.8721Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 463us/sample - loss: 0.6259 - acc: 0.8547\n",
            "1563/1563 [==============================] - 85s 55ms/step - loss: 0.5882 - acc: 0.8720 - val_loss: 0.6394 - val_acc: 0.8547\n",
            "Learning rate:  0.001\n",
            "Epoch 43/200\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.5827 - acc: 0.8730Epoch 1/200\n",
            "10000/1563 [===============================================================================================================================================================================================] - 5s 467us/sample - loss: 0.7679 - acc: 0.8117\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 0.5827 - acc: 0.8730 - val_loss: 0.8280 - val_acc: 0.8117\n",
            "Learning rate:  0.001\n",
            "Epoch 44/200\n",
            " 405/1563 [======>.......................] - ETA: 59s - loss: 0.5711 - acc: 0.8764 Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKcZDprY63fS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_no_bn = resnet_no_BN(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model_no_bn.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHbEPcxW88OR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_no_bn.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=1,\n",
        "                        callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH0_4Xs1KCfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evalutate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tewnLyYpKGex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_no_bn.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}