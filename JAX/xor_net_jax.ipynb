{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "8j43XX1UlZmW",
    "outputId": "c0ad4606-1f29-4cbf-81ce-4304db972db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (0.1.41)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jax) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from jax) (3.7.1)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jax) (1.16.4)\n",
      "Requirement already satisfied: fastcache in /usr/local/lib/python3.6/dist-packages (from jax) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jax) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->jax) (41.0.1)\n",
      "Collecting jaxlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/09/b7714a1f24e9514b05f0051a321ca489623f080a5f7a7974a3d0dd7c4830/jaxlib-0.1.23-cp36-none-manylinux1_x86_64.whl (24.1MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2MB 1.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.16.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.12.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from jaxlib) (3.7.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->jaxlib) (41.0.1)\n",
      "Installing collected packages: jaxlib\n",
      "Successfully installed jaxlib-0.1.23\n"
     ]
    }
   ],
   "source": [
    "!pip install jax\n",
    "!pip install jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rQWn46tl7Do"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6uXUtb5Nl1P-"
   },
   "outputs": [],
   "source": [
    "# Sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Computes our network's output\n",
    "def net(params, x):\n",
    "    w1, b1, w2, b2 = params\n",
    "    hidden = np.tanh(np.dot(w1, x) + b1)\n",
    "    return sigmoid(np.dot(w2, hidden) + b2)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def loss(params, x, y):\n",
    "    out = net(params, x)\n",
    "    cross_entropy = -y * np.log(out) - (1 - y)*np.log(1 - out)\n",
    "    return cross_entropy\n",
    "\n",
    "# Utility function for testing whether the net produces the correct\n",
    "# output for all possible inputs\n",
    "def test_all_inputs(inputs, params):\n",
    "    predictions = [int(net(params, inp) > 0.5) for inp in inputs]\n",
    "    for inp, out in zip(inputs, predictions):\n",
    "        print(inp, '->', out)\n",
    "    return (predictions == [onp.bitwise_xor(*inp) for inp in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14yM6DDcmP_R"
   },
   "outputs": [],
   "source": [
    "def initial_params():\n",
    "    return [\n",
    "        onp.random.randn(3, 2),  # w1\n",
    "        onp.random.randn(3),  # b1\n",
    "        onp.random.randn(3),  # w2\n",
    "        onp.random.randn(),  #b2\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "q_imBIi_mULR",
    "outputId": "d3767677-193c-43a5-ad5c-20fbb0a18dc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n",
      "Iteration 200\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "Iteration 300\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n",
      "Iteration 400\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "loss_grad = jax.grad(loss)\n",
    "\n",
    "\n",
    "# Stochastic gradient descent learning rate\n",
    "learning_rate = 1.\n",
    "# All possible inputs\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Initialize parameters randomly\n",
    "params = initial_params()\n",
    "\n",
    "for n in itertools.count():\n",
    "    # Grab a single random input\n",
    "    x = inputs[onp.random.choice(inputs.shape[0])]\n",
    "    # Compute the target output\n",
    "    y = onp.bitwise_xor(*x)\n",
    "    # Get the gradient of the loss for this input/output pair\n",
    "    grads = loss_grad(params, x, y)\n",
    "    # Update parameters via gradient descent\n",
    "    params = [param - learning_rate * grad\n",
    "              for param, grad in zip(params, grads)]\n",
    "    # Every 100 iterations, check whether we've solved XOR\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "1taa0bz_mYLW",
    "outputId": "14af43ca-1ac5-4d26-96de-a14ace1e6fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 7.34 ms per loop\n",
      "1000 loops, best of 3: 371 µs per loop\n"
     ]
    }
   ],
   "source": [
    "# Time the original gradient function\n",
    "%timeit loss_grad(params, x, y)\n",
    "loss_grad = jax.jit(jax.grad(loss))\n",
    "# Run once to trigger JIT compilation\n",
    "loss_grad(params, x, y)\n",
    "%timeit loss_grad(params, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "7fZHAdwqmlEY",
    "outputId": "8a59f7f1-844c-4273-9b7f-85b6b9e9d64c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "params = initial_params()\n",
    "\n",
    "for n in itertools.count():\n",
    "    x = inputs[onp.random.choice(inputs.shape[0])]\n",
    "    y = onp.bitwise_xor(*x)\n",
    "    grads = loss_grad(params, x, y)\n",
    "    params = [param - learning_rate * grad\n",
    "              for param, grad in zip(params, grads)]\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "6aIWRPSpmyqB",
    "outputId": "a9852a44-24f9-4810-b17d-ccb0c748c4a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 0\n",
      "[1 1] -> 0\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "loss_grad = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0, 0), out_axes=0))\n",
    "\n",
    "params = initial_params()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for n in itertools.count():\n",
    "    # Generate a batch of inputs\n",
    "    x = inputs[onp.random.choice(inputs.shape[0], size=batch_size)]\n",
    "    y = onp.bitwise_xor(x[:, 0], x[:, 1])\n",
    "    # The call to loss_grad remains the same!\n",
    "    grads = loss_grad(params, x, y)\n",
    "    # Note that we now need to average gradients over the batch\n",
    "    params = [param - learning_rate * np.mean(grad, axis=0)\n",
    "              for param, grad in zip(params, grads)]\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oGgvWTE3nibj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "xor_net_jax.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
